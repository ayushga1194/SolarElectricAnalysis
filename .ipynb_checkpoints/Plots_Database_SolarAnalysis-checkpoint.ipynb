{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utility Functions\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "\n",
    "def create_connection(db_file, delete_db=False):\n",
    "    import os\n",
    "    if delete_db and os.path.exists(db_file):\n",
    "        os.remove(db_file)\n",
    "\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        conn.execute(\"PRAGMA foreign_keys = 1\")\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "\n",
    "    return conn\n",
    "\n",
    "\n",
    "def create_table(conn, create_table_sql, drop_table_name=None):\n",
    "    \n",
    "    if drop_table_name: # You can optionally pass drop_table_name to drop the table. \n",
    "        try:\n",
    "            c = conn.cursor()\n",
    "            c.execute(\"\"\"DROP TABLE IF EXISTS %s\"\"\" % (drop_table_name))\n",
    "        except Error as e:\n",
    "            print(e)\n",
    "    \n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(create_table_sql)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "        \n",
    "def execute_sql_statement(sql_statement, conn):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql_statement)\n",
    "\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete old database file\n",
    "normalized_database_filename = 'normalized1.db'\n",
    "conn = create_connection(normalized_database_filename, delete_db=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the dataframe from file\n",
    "df=pd.read_csv('solar.csv')\n",
    "df1=df.copy()\n",
    "df2=df.copy()\n",
    "df3=df.copy()\n",
    "df4=df.copy()\n",
    "df5=df.copy()\n",
    "df6=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Project Table\n",
    "#Dataframe ColumnName\tColumn Name in DB Table\n",
    "'''\n",
    "Project Number(Common)\t1(ProjectID)[Primary Key]\n",
    "Sector\t7(Sector)\n",
    "Program Type\t8(Program Type)\n",
    "Solicitation\t9(Solicitation)\n",
    "Electric Utility\t10(Electric Utility)\n",
    "Purchase Type\t11(PurchaseType)\n",
    "Project Status\t14(Project Status)\n",
    "Contractor\t15(Contractor)\n",
    "'''\n",
    "\n",
    "def step1_create_project_table(normalized_database_filename):\n",
    "    ll=[]\n",
    "    for l in df.iterrows():\n",
    "        ll.append((str(l[1][1]),str(l[1][7]),str(l[1][8]),str(l[1][9]),str(l[1][10]),\n",
    "                   str(l[1][11]),str(l[1][14]),str(l[1][15])))\n",
    "    \n",
    "    conn_norm=create_connection(normalized_database_filename)\n",
    "    create_table(conn_norm,\"\"\"create table Project(ProjectID TEXT NOT NULL PRIMARY KEY,Sector TEXT,\n",
    "    Program Type TEXT, Solicitation TEXT, ElectricUtility TEXT, PurchaseType TEXT, ProjectStatus TEXT,\n",
    "    Contractor TEXT,UNIQUE(ProjectID))\"\"\",\"Project\")\n",
    "    conn_norm.commit()\n",
    "    cur = conn_norm.cursor()\n",
    "    cur.executemany(\"INSERT INTO Project VALUES (?, ?, ?, ?, ?, ?, ?, ?)\", ll)\n",
    "    conn_norm.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying the table creation & insertion of Project\n",
    "normalized_database_filename = 'normalized1.db'\n",
    "step1_create_project_table(normalized_database_filename)\n",
    "conn = create_connection(normalized_database_filename)\n",
    "cur = conn.cursor()\n",
    "df = pd.read_sql_query(\"\"\"SELECT * FROM Project\"\"\", conn)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the ProjectLocation Table\n",
    "#Dataframe ColumnName\tColumn Name in DB Table\n",
    "'''\n",
    "ProjectLocationID\tPrimary Key\n",
    "Project Number(Common)\t1(ProjectID)[Foreign Key]\n",
    "City\t3(City)\n",
    "County\t4(County)\n",
    "Zip Code\t6(Zip Code)\n",
    "'''\n",
    "\n",
    "def step2_create_project_location_table(normalized_database_filename):\n",
    "    ll1=[]\n",
    "    df=pd.read_csv('solar.csv')\n",
    "    for l in df1.iterrows():\n",
    "        ll1.append((l[0]+1,l[1][6],str(l[1][1]),l[1][3],l[1][4])) \n",
    "\n",
    "    conn_norm=create_connection(normalized_database_filename)\n",
    "    create_table(conn_norm,\"\"\"create table ProjectLocation(ProjectLocationID INTEGER NOT NULL PRIMARY KEY, \n",
    "    ZipCode TEXT NOT NULL, ProjectID TEXT NOT NULL,City TEXT,County TEXT,\n",
    "    FOREIGN KEY(ProjectID) REFERENCES Project(ProjectID), UNIQUE(ProjectLocationID))\"\"\",\"ProjectLocation\")\n",
    "    conn_norm.commit()\n",
    "    cur = conn_norm.cursor()\n",
    "    cur.executemany(\"INSERT INTO ProjectLocation VALUES (?, ?, ?, ?, ?)\", ll1)\n",
    "    conn_norm.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying the table creation & insertion of ProjectLocation\n",
    "normalized_database_filename = 'normalized1.db'\n",
    "step2_create_project_location_table(normalized_database_filename)\n",
    "conn = create_connection(normalized_database_filename)\n",
    "cur = conn.cursor()\n",
    "df = pd.read_sql_query(\"\"\"SELECT * FROM ProjectLocation\"\"\", conn)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the InverterDetails Table\n",
    "#Dataframe ColumnName\tColumn Name in DB Table\n",
    "'''\n",
    "InverterDetailsID\tPrimary Key\n",
    "Project Number(Common)\t1(ProjectID)[Foreign Key]\n",
    "Primary Inverter Manufacturer\t16(PriInvetMft)\n",
    "Primary Inverter Model Number\t17(PriInvetModNo)\n",
    "Total Inverter Quantity\t18(TotalInvtQt)\n",
    "'''\n",
    "\n",
    "def step3_create_inverter_details_table(data_filename, normalized_database_filename):\n",
    "    ll=[]\n",
    "    df=pd.read_csv('solar.csv')\n",
    "    for l in df2.iterrows():\n",
    "        ll.append((l[0]+1,l[1][1],l[1][16],l[1][17],l[1][18])) \n",
    "    \n",
    "    conn_norm=create_connection(normalized_database_filename)\n",
    "    create_table(conn_norm,\"\"\"create table InverterDetails(InverterDetailsID INTEGER NOT NULL PRIMARY KEY, \n",
    "    ProjectID TEXT NOT NULL, PriInvetMft TEXT, PriInvetModNo TEXT,TotalInvtQt TEXT,\n",
    "    FOREIGN KEY(ProjectID) REFERENCES Project(ProjectID), UNIQUE(InverterDetailsID))\"\"\",\"InverterDetails\")\n",
    "    conn_norm.commit()\n",
    "    cur = conn_norm.cursor()\n",
    "    cur.executemany(\"INSERT INTO InverterDetails VALUES (?, ?, ?, ?, ?)\", ll)\n",
    "    conn_norm.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying the table creation & insertion of InverterDetails\n",
    "normalized_database_filename = 'normalized1.db'\n",
    "data_filename = 'solar.csv'\n",
    "step3_create_inverter_details_table(data_filename, normalized_database_filename)\n",
    "conn = create_connection(normalized_database_filename)\n",
    "cur = conn.cursor()\n",
    "df = pd.read_sql_query(\"\"\"SELECT * FROM InverterDetails\"\"\", conn)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the PVModuleDetails Table\n",
    "#Dataframe ColumnName\tColumn Name in DB Table\n",
    "'''\n",
    "PVModuleDetailsID\tPrimary Key\n",
    "Project Number(Common)\t1(ProjectID)[Foreign Key]\n",
    "Primary PV Module Manufacturer\t19(PriPVMft)\n",
    "PV Module Model Number\t20(PriPVModNo)\n",
    "Total PV Module Quantity\t21(TotalPVQt)\n",
    "'''\n",
    "\n",
    "def step4_create_pvmodule_details_table(data_filename, normalized_database_filename):\n",
    "    ll=[]\n",
    "    for l in df3.iterrows():\n",
    "        ll.append((l[0]+1,l[1][1],l[1][19],l[1][20],l[1][21]))\n",
    "    \n",
    "    conn_norm=create_connection(normalized_database_filename)\n",
    "    create_table(conn_norm,\"\"\"create table PVModuleDetails(PVModuleDetailsID INTEGER NOT NULL PRIMARY KEY, \n",
    "    ProjectID TEXT NOT NULL, PriPVMft TEXT, PriPVModNo TEXT,TotalPVQt TEXT, \n",
    "    FOREIGN KEY(ProjectID) REFERENCES Project(ProjectID), UNIQUE(PVModuleDetailsID))\"\"\",\"PVModuleDetails\")\n",
    "    conn_norm.commit()\n",
    "    cur = conn_norm.cursor()\n",
    "    cur.executemany(\"INSERT INTO PVModuleDetails VALUES (?, ?, ?, ?, ?)\", ll)\n",
    "    conn_norm.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying the table creation & insertion of PVModuleDetails\n",
    "normalized_database_filename = 'normalized1.db'\n",
    "data_filename = 'solar.csv'\n",
    "step4_create_pvmodule_details_table(data_filename, normalized_database_filename)\n",
    "conn = create_connection(normalized_database_filename)\n",
    "cur = conn.cursor()\n",
    "df = pd.read_sql_query(\"\"\"SELECT * FROM PVModuleDetails\"\"\", conn)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the ProjectTimeline Table\n",
    "#Dataframe ColumnName\tColumn Name in DB Table\n",
    "'''\n",
    "ProjectTimelineID\tPrimary Key\n",
    "Project Number(Common)\t1(ProjectID)[Foreign Key]\n",
    "Date Application Received\t11(DtAppReceived)\n",
    "Date Completed\t12(DtCompleted)\n",
    "'''\n",
    "\n",
    "def step5_create_project_timeline_table(data_filename, normalized_database_filename):\n",
    "    ll=[]\n",
    "    for l in df4.iterrows():\n",
    "        ll.append((l[0]+1,l[1][1],l[1][12],l[1][13]))\n",
    "    \n",
    "    \n",
    "    conn_norm=create_connection(normalized_database_filename)\n",
    "    create_table(conn_norm,\"\"\"create table ProjectTimeline(ProjectTimelineID INTEGER NOT NULL PRIMARY KEY, ProjectID \n",
    "    TEXT NOT NULL, DtAppReceived TEXT, DtCompleted TEXT,\n",
    "    FOREIGN KEY(ProjectID) REFERENCES Project(ProjectID), UNIQUE(ProjectTimelineID))\"\"\",\"ProjectTimeline\")\n",
    "    conn_norm.commit()\n",
    "    cur = conn_norm.cursor()\n",
    "    cur.executemany(\"INSERT INTO ProjectTimeline VALUES (?, ?, ?, ?)\", ll)\n",
    "    conn_norm.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying the table creation & insertion of ProjectTimeline\n",
    "normalized_database_filename = 'normalized1.db'\n",
    "data_filename = 'solar.csv'\n",
    "step5_create_project_timeline_table(data_filename, normalized_database_filename)\n",
    "conn = create_connection(normalized_database_filename)\n",
    "cur = conn.cursor()\n",
    "df = pd.read_sql_query(\"\"\"SELECT * FROM ProjectTimeline\"\"\", conn)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the ProjectCost Table\n",
    "#Dataframe ColumnName\tColumn Name in DB Table\n",
    "'''\n",
    "ProjectCostID\tPrimary Key\n",
    "Project Number(Common)\t1(ProjectID)[Foreign Key]\n",
    "Project Cost\t22(ProjectCost)\n",
    "$Incentive\t23(Incentive)\n",
    "Remote Net Metering\t26(RemNetMet)\n",
    "Affordable Solar\t27(AffSolar)\n",
    "Green Jobs Green New York Participant\t29(GreenCertified)\n",
    "'''\n",
    "\n",
    "def step6_create_project_cost_table(data_filename, normalized_database_filename):\n",
    "    ll=[]\n",
    "    for l in df5.iterrows():\n",
    "        ll.append((l[0]+1,l[1][1],l[1][22],l[1][23],l[1][26],l[1][27],l[1][29]))\n",
    "    \n",
    "    conn_norm=create_connection(normalized_database_filename)\n",
    "    create_table(conn_norm,\"\"\"create table ProjectCost(ProjectCostID INTEGER NOT NULL PRIMARY KEY, ProjectID \n",
    "    TEXT NOT NULL, ProjectCost TEXT, Incentive TEXT, RemNetMet TEXT, AffSolar TEXT, GreenCertified TEXT,\n",
    "    FOREIGN KEY(ProjectID) REFERENCES Project(ProjectID), UNIQUE(ProjectCostID))\"\"\",\"ProjectCost\")\n",
    "    conn_norm.commit()\n",
    "    cur = conn_norm.cursor()\n",
    "    cur.executemany(\"INSERT INTO ProjectCost VALUES (?, ?, ?, ?, ?, ?, ?)\", ll)\n",
    "    conn_norm.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying the table creation & insertion of ProjectCost\n",
    "normalized_database_filename = 'normalized1.db'\n",
    "data_filename = 'solar.csv'\n",
    "step6_create_project_cost_table(data_filename, normalized_database_filename)\n",
    "conn = create_connection(normalized_database_filename)\n",
    "cur = conn.cursor()\n",
    "df = pd.read_sql_query(\"\"\"SELECT * FROM ProjectCost\"\"\", conn)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the ProjectProd Table\n",
    "#Dataframe ColumnName\tColumn Name in DB Table\n",
    "'''\n",
    "ProjectProdID\tPrimary Key\n",
    "Project Number(Common)\t1(ProjectID)[Foreign Key]\n",
    "Total Nameplate kW DC\t24(TotNamePlateKWDC)\n",
    "Expected KWh Annual Production\t25(ExpectKWhAnnProd)\n",
    "Community Distributed Generation\t28(CommDistGener)\n",
    "'''\n",
    "\n",
    "def step7_create_project_prod_table(data_filename, normalized_database_filename):\n",
    "    ll=[]\n",
    "    for l in df6.iterrows():\n",
    "        #if l[1][24]==0 or l[1][24]==np.nan: \n",
    "        #    ll.append((l[0]+1,l[1][1],l[1][24],l[1][25],l[1][28],0))\n",
    "        #else:\n",
    "            #pc=float(l[1][22])\n",
    "            #tnkwhdc=float(l[1][24])\n",
    "            #eff=1-(pc/tnkwhdc)\n",
    "            #ll.append((l[0]+1,l[1][1],l[1][24],l[1][25],l[1][28],eff*100))\n",
    "        ll.append((l[0]+1,l[1][1],l[1][24],l[1][25],l[1][28]))\n",
    "    \n",
    "    conn_norm=create_connection(normalized_database_filename)\n",
    "    create_table(conn_norm,\"\"\"create table ProjectProd(ProjectProdID INTEGER NOT NULL PRIMARY KEY, ProjectID \n",
    "    TEXT NOT NULL, TotNamePlateKWDC TEXT, ExpectKWhAnnProd TEXT, CommDistGener TEXT,\n",
    "    FOREIGN KEY(ProjectID) REFERENCES Project(ProjectID), UNIQUE(ProjectProdID))\"\"\",\"ProjectProd\")\n",
    "    conn_norm.commit()\n",
    "    cur = conn_norm.cursor()\n",
    "    cur.executemany(\"INSERT INTO ProjectProd VALUES (?, ?, ?, ?, ?)\", ll)\n",
    "    conn_norm.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying the table creation & insertion of ProjectProd\n",
    "normalized_database_filename = 'normalized1.db'\n",
    "data_filename = 'solar.csv'\n",
    "step7_create_project_prod_table(data_filename, normalized_database_filename)\n",
    "conn = create_connection(normalized_database_filename)\n",
    "cur = conn.cursor()\n",
    "df = pd.read_sql_query(\"\"\"SELECT * FROM ProjectProd\"\"\", conn)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting County Code to FIPS Code for Populating the Choropleth Map\n",
    "def county_to_3_digit_fips_code(df_counties_fips):  \n",
    "    ll_counties_fips=[]\n",
    "    for ele in df_counties_fips.iterrows():\n",
    "        num=str(ele[1][0])\n",
    "        ll_counties_fips.append((num,ele[1][1]))\n",
    "    return ll_counties_fips\n",
    "\n",
    "def df_county_to_fips_code(df, ll_counties_fips):\n",
    "    df[pd.isnull(df['County'])]\n",
    "    ll_df_fips=[]\n",
    "    for ele in df['County']:\n",
    "        flag=0\n",
    "        for i in range(len(ll_counties_fips)):\n",
    "            #ele=str(ele)\n",
    "            if type(ele)==float:\n",
    "                ele=str(ele)\n",
    "                ele=ele.strip()\n",
    "            ele=ele.replace('.','')\n",
    "            if ele==ll_counties_fips[i][1]:\n",
    "                #print(type(ll[i][1]))\n",
    "                ll_df_fips.append(ll_counties_fips[i][0])\n",
    "                flag=1\n",
    "        if flag==0:\n",
    "            ll_df_fips.append('125')\n",
    "    return ll_df_fips\n",
    "\n",
    "def year_filtered_dataframe(col,table,year):\n",
    "    sql_statement=\"\"\"select County, pt.DtAppReceived, {0} from \n",
    "            ProjectTimeline AS pt \n",
    "            INNER JOIN \n",
    "            {1} AS pd\n",
    "            on pt.ProjectID=pd.ProjectID\n",
    "            INNER JOIN \n",
    "            ProjectLocation AS pl\n",
    "            on pt.ProjectID=pl.ProjectID\n",
    "            where cast(substr(pt.DtAppReceived,length(pt.DtAppReceived)-3,4) as INTEGER)<={2}\"\"\"\n",
    "    sql_statement = sql_statement.format(col,table,year)\n",
    "    normalized_database_filename = 'normalized1.db'\n",
    "    conn = create_connection(normalized_database_filename)\n",
    "    cur = conn.cursor()\n",
    "    df_yr = pd.read_sql_query(sql_statement, conn)\n",
    "    df_yr = df_yr.dropna()\n",
    "    df_yr = df_yr.replace('','0')\n",
    "    df_yr[col]= df_yr[col].astype(float) \n",
    "    return df_yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def df_year_filtered(col,table,year):\n",
    "    #Filtering the year wise data\n",
    "    df_year = year_filtered_dataframe(col,table,year)\n",
    "    return df_year\n",
    "\n",
    "def ll_counties_fips():\n",
    "    #Reading the dataframes for the solar and counties\n",
    "    #df['Expected KWh Annual Production']=df['Expected KWh Annual Production'].replace(np.nan,0)\n",
    "    df_counties_fips = pd.read_csv('Counties.csv',error_bad_lines=False)\n",
    "\n",
    "    #Converting the county to 3 digit fips code\n",
    "    ll_counties_fips = county_to_3_digit_fips_code(df_counties_fips)\n",
    "    return ll_counties_fips\n",
    "\n",
    "def ll_df_fips(df, ll_counties_fips):\n",
    "    #Adding the fips code of all the counties in the dataframe\n",
    "    ll_df_fips = df_county_to_fips_code(df,ll_counties_fips)\n",
    "    return ll_df_fips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
    "    counties = json.load(response)\n",
    "import plotly.express as px\n",
    "yr=int(input(\"Enter year:\"))\n",
    "col=str(input(\"Enter column:\"))\n",
    "table=str(input(\"Enter table:\"))\n",
    "for year in range(yr,yr+1): \n",
    "    df_fips_expectedAnnProdKwh=pd.DataFrame()\n",
    "    df_year=df_year_filtered(col,table,year)\n",
    "    ll=ll_counties_fips()\n",
    "    ll2=ll_df_fips(df_year, ll)\n",
    "\n",
    "    df_fips_expectedAnnProdKwh['solarprod']=df_year[col]\n",
    "\n",
    "    df_fips_expectedAnnProdKwh['fips']=ll2\n",
    "    df_fips_expectedAnnProdKwh['sum_kwh_prod']=df_fips_expectedAnnProdKwh.groupby('fips').transform('sum')\n",
    "    df_fips_expectedAnnProdKwh=df_fips_expectedAnnProdKwh.sort_values(by=['fips'])\n",
    "    df_fips_expectedAnnProdKwh=df_fips_expectedAnnProdKwh.drop(['solarprod'],axis=1)\n",
    "    \n",
    "    #Using Choropleth\n",
    "    fig = px.choropleth(df_fips_expectedAnnProdKwh, geojson=counties, locations='fips', color='sum_kwh_prod',\n",
    "                           color_continuous_scale=\"Viridis\",\n",
    "                           range_color=(0, max(df_fips_expectedAnnProdKwh['sum_kwh_prod'])),\n",
    "                           scope=\"usa\",\n",
    "                           labels={'sum_kwh_prod':col}\n",
    "                          )\n",
    "    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Electric Utility proportion in Total Name Plate Kwh DC\n",
    "sql_statement = \"\"\"SELECT ElectricUtility AS eu,\n",
    "    SUM(TotNamePlateKWDC) AS DC_Capacity\n",
    "    FROM Project INNER JOIN PROJECTPROD\n",
    "    ON PROJECT.PROJECTID=PROJECTPROD.PROJECTID\n",
    "    GROUP BY eu ORDER  BY DC_Capacity DESC\"\"\"\n",
    "\n",
    "normalized_database_filename = 'normalized1.db'\n",
    "conn = create_connection(normalized_database_filename)\n",
    "cur = conn.cursor()\n",
    "df_yr = pd.read_sql_query(sql_statement, conn)\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "df_yr=df_yr.replace('nan','Others')\n",
    "separated=(.1,0,0.1,0,0.1,0,0.1,0)\n",
    "plt.pie(df_yr['DC_Capacity'].tolist(), labels=df_yr[\"eu\"],\n",
    "        autopct='%1.0f%%', shadow=False, startangle=120,explode=separated)\n",
    "#plt.legend(df_yr['DC_Generated'].tolist(), labels=df_yr[\"eu\"], loc=\"lower left\")\n",
    "#plt.axis('equal')\n",
    "plt.rcParams['font.size'] = 20\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of Purchase Type \n",
    "sql_statement = \"\"\"SELECT PurchaseType AS pt,\n",
    "    COUNT(ProjectID) AS IDS\n",
    "    FROM Project \n",
    "    GROUP BY pt\"\"\"\n",
    "\n",
    "normalized_database_filename = 'normalized1.db'\n",
    "conn = create_connection(normalized_database_filename)\n",
    "cur = conn.cursor()\n",
    "df_yr = pd.read_sql_query(sql_statement, conn)\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "df_yr=df_yr.replace('nan','Others')\n",
    "separated=(.1,0,0.1,0)\n",
    "plt.pie(df_yr['IDS'].tolist(), labels=df_yr[\"pt\"],\n",
    "        autopct='%1.0f%%', shadow=False, startangle=290,explode=separated)\n",
    "#plt.legend(df_yr['DC_Generated'].tolist(), labels=df_yr[\"eu\"], loc=\"lower left\")\n",
    "#plt.axis('equal')\n",
    "plt.rcParams['font.size'] = 20\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changes in the Project Cost and Total KwhDC generated\n",
    "year=2020\n",
    "x=[]\n",
    "y=[]\n",
    "year1=[]\n",
    "for yr in range(2001,year+1):\n",
    "    sql_statement = \"\"\"SELECT SUM(ProjectCost) AS pc,\n",
    "    SUM(TotNamePlateKWDC) AS DC_Capacity\n",
    "    FROM ProjectCost INNER JOIN PROJECTPROD\n",
    "    ON \n",
    "    PROJECTCOST.PROJECTID=PROJECTPROD.PROJECTID\n",
    "    INNER JOIN ProjectTimeline AS pt\n",
    "    ON\n",
    "    pt.PROJECTID=PROJECTPROD.PROJECTID\n",
    "    where cast(substr(pt.DtAppReceived,length(pt.DtAppReceived)-3,4) as INTEGER)<={0}\"\"\"\n",
    "    \n",
    "    sql_statement1=\"\"\"SELECT SUM(ProjectCost) AS pc,\n",
    "    SUM(TotNamePlateKWDC) AS DC_Capacity\n",
    "    FROM ProjectCost INNER JOIN PROJECTPROD\n",
    "    ON \n",
    "    PROJECTCOST.PROJECTID=PROJECTPROD.PROJECTID\n",
    "    INNER JOIN ProjectTimeline AS pt\n",
    "    ON\n",
    "    pt.PROJECTID=PROJECTPROD.PROJECTID\n",
    "    where cast(substr(pt.DtAppReceived,length(pt.DtAppReceived)-3,4) as INTEGER)<={0}\"\"\"\n",
    "    sql_statement = sql_statement.format(yr)\n",
    "    sql_statement1 = sql_statement1.format(yr-1)\n",
    "    normalized_database_filename = 'normalized1.db'\n",
    "    conn = create_connection(normalized_database_filename)\n",
    "    cur = conn.cursor()\n",
    "    df_yr = pd.read_sql_query(sql_statement, conn)\n",
    "    df_yr1 = pd.read_sql_query(sql_statement1, conn)\n",
    "    x.append(df_yr['pc']-df_yr1['pc'])\n",
    "    y.append(df_yr['DC_Capacity']-df_yr1['DC_Capacity'])\n",
    "    year1.append(yr)\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(year1, x, '-ok',color='Blue')\n",
    "plt.xlabel('Year')\n",
    "plt.xticks(np.arange(min(year1), max(year1)+1, 2.0))\n",
    "plt.ylabel('Project Cost Change')\n",
    "plt.title('ProjectCost Change over the years')\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(year1,y, '-ok', color='Red')\n",
    "plt.xticks(np.arange(min(year1), max(year1)+1, 2.0))\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total Name Plate DC Change')\n",
    "plt.title('Total Name Plate DC Change over the years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Highest Solar Producing powerplant in each county\n",
    "sql_statement = \"\"\"SELECT MAX(TotNamePlateKWDC),ProjectProd.ProjectID,RemNetMet,GreenCertified,AffSolar FROM\n",
    "                    ProjectProd INNER JOIN ProjectLocation on \n",
    "                    ProjectProd.projectid=ProjectLocation.projectid \n",
    "                    INNER JOIN ProjectCost\n",
    "                    ON ProjectProd.projectid=ProjectCost.projectid\n",
    "                    group by County order by TotNamePlateKWDC desc \"\"\"\n",
    "\n",
    "normalized_database_filename = 'normalized1.db'\n",
    "conn = create_connection(normalized_database_filename)\n",
    "cur = conn.cursor()\n",
    "df_yr = pd.read_sql_query(sql_statement, conn)\n",
    "df=pd.read_csv('solar.csv')\n",
    "ll2=[]\n",
    "c=[]\n",
    "remnetmt=[]\n",
    "#print(df_yr[df_yr['GreenCertified']=='Yes'].index)---->44th project is Green certified\n",
    "#print(df_yr[df_yr['AffSolar']=='Yes'].index)----->No project is Affordable Solar\n",
    "for ele in df_yr['RemNetMet']:\n",
    "    remnetmt.append(ele)\n",
    "for ele in df_yr['ProjectID']:\n",
    "    if ele.isnumeric():\n",
    "        loc=df[df['Project Number']==float(ele)]['Location 1'].tolist()\n",
    "    else:\n",
    "        loc=df[df['Project Number']==str(ele)]['Location 1'].tolist()\n",
    "    if len(loc)==0:\n",
    "        loc=df[df['Project Number']==str(ele)]['Location 1'].tolist()\n",
    "    ll2.append(loc)\n",
    "    \n",
    "import folium\n",
    "from folium import plugins\n",
    "ll1=[]\n",
    "ll3=[]\n",
    "for ele in ll2:\n",
    "    ll=ele[0].split('\\n')\n",
    "    ll3.append(ll[0])\n",
    "    ll[1]=ll[1].strip('()')\n",
    "    a=ll[1].split(',')\n",
    "    a[1]=a[1].strip()\n",
    "    ll1.append(a)\n",
    "\n",
    "map = folium.Map(location=ll1[25], zoom_start=6.5)\n",
    "i=0\n",
    "k=0\n",
    "for k in range(0,len(ll1)):\n",
    "    ele=remnetmt[k]\n",
    "    if k==44:\n",
    "        folium.Marker(location = [ll1[k][0], ll1[k][1]],\n",
    "        popup = ll3[k],\n",
    "        icon = folium.Icon(color = 'green')).add_to(map)\n",
    "    else:\n",
    "        if ele=='Yes':\n",
    "            folium.Marker(location = [ll1[k][0], ll1[k][1]],\n",
    "            popup = ll3[k],\n",
    "            icon = folium.Icon(color = 'gray')).add_to(map)\n",
    "        else:\n",
    "            folium.Marker(location = [ll1[k][0], ll1[k][1]],\n",
    "            popup = ll3[k],\n",
    "            icon = folium.Icon(color = 'orange')).add_to(map)\n",
    "    k+=1\n",
    "map"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
